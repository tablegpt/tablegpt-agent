{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>tablegpt-agent is a pre-built agent for TableGPT2 (huggingface), a series of LLMs for table-based question answering. This agent is built on top of the Langgraph library and provides a user-friendly interface for interacting with TableGPT2.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ul> <li>Tutorials<ul> <li>Quickstart</li> <li>Chat on Tabular Data</li> <li>Continue Analysis on Generated Charts</li> </ul> </li> <li>How-To Guides<ul> <li>Enhance TableGPT Agent with RAG</li> <li>Persist Messages</li> <li>Incluster Code Execution</li> <li>Normalize Datasets</li> </ul> </li> <li>Explanation<ul> <li>Agent Workflow</li> <li>File Reading</li> </ul> </li> <li>Reference</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Thank you for your interest in TableGPT Agent. For more information on contributing, please see the contributing guide.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We extend our sincere gratitude to all contributors and collaborators who played a pivotal role in the development of tablegpt-agent. Special thanks to our team members and the open-source community, whose insights and feedback were invaluable throughout the project.</p> <p>Thank you to our early users for their suggestions and engagement, which have greatly helped in refining and enhancing this tool.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Creates a state graph for processing datasets.</p> <p>This function orchestrates the creation of a workflow for handling table data. It sets up nodes for reading files and analyzing data based on provided parameters. The graph dynamically routes based on the presence of file attachments in the input state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Runnable</code> <p>The primary language model for processing user input.</p> required <code>pybox_manager</code> <code>BasePyBoxManager</code> <p>A python code sandbox delegator, used to execute the data analysis code generated by llm.</p> required <code>session_id</code> <code>str | None</code> <p>An optional session identifier used to associate with <code>pybox</code>. Defaults to None.</p> <code>None</code> <code>workdir</code> <code>Path | None</code> <p>The working directory for <code>pybox</code> operations. Defaults to None.</p> <code>None</code> <code>error_trace_cleanup</code> <code>bool</code> <p>Flag to clean up error traces. Defaults to False.</p> <code>False</code> <code>nlines</code> <code>int | None</code> <p>Number of lines to read for preview. Defaults to None.</p> <code>None</code> <code>vlm</code> <code>BaseLanguageModel | None</code> <p>Optional vision language model for processing images. Defaults to None.</p> <code>None</code> <code>safety_llm</code> <code>Runnable | None</code> <p>Model used for safety classification of inputs. Defaults to None.</p> <code>None</code> <code>dataset_retriever</code> <code>BaseRetriever | None</code> <p>Component to retrieve datasets. Defaults to None.</p> <code>None</code> <code>normalize_llm</code> <code>BaseLanguageModel | None</code> <p>Model for data normalization tasks. Defaults to None.</p> <code>None</code> <code>locate</code> <code>str | None</code> <p>The locale of the user. Defaults to None.</p> required <code>checkpointer</code> <code>BaseCheckpointSaver | None</code> <p>Component for saving checkpoints. Defaults to None.</p> <code>None</code> <code>trim_message_method</code> <code>Literal['default', 'token_count']</code> <p>Determines the method used to trim the message. Defaults to \"default\".</p> <code>'default'</code> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CompiledStateGraph</code> <code>CompiledStateGraph</code> <p>A compiled state graph representing the table processing workflow.</p> Source code in <code>src/tablegpt/agent/__init__.py</code> <pre><code>def create_tablegpt_graph(\n    llm: BaseLanguageModel,\n    pybox_manager: BasePyBoxManager,\n    *,\n    session_id: str | None = None,\n    workdir: Path | None = None,\n    error_trace_cleanup: bool = False,\n    nlines: int | None = None,\n    vlm: BaseLanguageModel | None = None,\n    safety_llm: Runnable | None = None,\n    dataset_retriever: BaseRetriever | None = None,\n    normalize_llm: BaseLanguageModel | None = None,\n    locale: str | None = None,\n    checkpointer: BaseCheckpointSaver | None = None,\n    trim_message_method: Literal[\"default\", \"token_count\"] = \"default\",\n    verbose: bool = False,\n) -&gt; CompiledStateGraph:\n    \"\"\"Creates a state graph for processing datasets.\n\n    This function orchestrates the creation of a workflow for handling table data.\n    It sets up nodes for reading files and analyzing data based on provided parameters.\n    The graph dynamically routes based on the presence of file attachments in the input state.\n\n    Args:\n        llm (Runnable): The primary language model for processing user input.\n        pybox_manager (BasePyBoxManager):  A python code sandbox delegator, used to execute the data analysis code generated by llm.\n        session_id (str | None, optional): An optional session identifier used to associate with `pybox`. Defaults to None.\n        workdir (Path | None, optional): The working directory for `pybox` operations. Defaults to None.\n        error_trace_cleanup (bool, optional): Flag to clean up error traces. Defaults to False.\n        nlines (int | None, optional): Number of lines to read for preview. Defaults to None.\n        vlm (BaseLanguageModel | None, optional): Optional vision language model for processing images. Defaults to None.\n        safety_llm (Runnable | None, optional): Model used for safety classification of inputs. Defaults to None.\n        dataset_retriever (BaseRetriever | None, optional): Component to retrieve datasets. Defaults to None.\n        normalize_llm (BaseLanguageModel | None, optional): Model for data normalization tasks. Defaults to None.\n        locate (str | None, optional): The locale of the user. Defaults to None.\n        checkpointer (BaseCheckpointSaver | None, optional): Component for saving checkpoints. Defaults to None.\n        trim_message_method (Literal[\"default\", \"token_count\"], optional): Determines the method used to trim the message. Defaults to \"default\".\n        verbose (bool, optional): Flag to enable verbose logging. Defaults to False.\n\n    Returns:\n        CompiledStateGraph: A compiled state graph representing the table processing workflow.\n    \"\"\"\n    workflow = StateGraph(AgentState)\n    file_reading_graph = create_file_reading_workflow(\n        nlines=nlines,\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        normalize_llm=normalize_llm,\n        locale=locale,\n        verbose=verbose,\n    )\n    data_analyze_graph = create_data_analyze_workflow(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        error_trace_cleanup=error_trace_cleanup,\n        vlm=vlm,\n        safety_llm=safety_llm,\n        dataset_retriever=dataset_retriever,\n        trim_message_method=trim_message_method,\n        verbose=verbose,\n    )\n\n    def router(state: AgentState) -&gt; str:\n        # Must have at least one message when entering this router\n        last_message = state[\"messages\"][-1]\n        if last_message.additional_kwargs.get(\"attachments\"):\n            return \"file_reading_graph\"\n        return \"data_analyze_graph\"\n\n    workflow.add_node(\"file_reading_graph\", file_reading_graph)\n    workflow.add_node(\"data_analyze_graph\", data_analyze_graph)\n\n    workflow.add_conditional_edges(START, router)\n    workflow.add_edge(\"file_reading_graph\", END)\n    workflow.add_edge(\"data_analyze_graph\", END)\n\n    return workflow.compile(checkpointer=checkpointer, debug=verbose)\n</code></pre>"},{"location":"explanation/agent-workflow/","title":"Agent Workflow","text":"<p>The Agent Workflow is the core functionality of the <code>tablegpt-agent</code>. It processes user input and generates appropriate responses. This workflow is similar to those found in most single-agent systems and consists of an agent and various tools. Specifically, the data analysis workflow includes:</p> <ul> <li>An Agent Powered by TableGPT2: This agent performs data analysis tasks.</li> <li>An IPython tool: This tool executes the generated code within a sandbox environment.</li> </ul> <p>Additionally, TableGPT Agent offers several optional plugins that extend the agent's functionality:</p> <ul> <li>A Visual Language Model that can be used to enhance summarization for data visualization tasks.</li> <li>A retriever that fetches information about the dataset, improving the quality and relevance of the generated code.</li> <li>A safety mechanism that protects the system from toxic inputs.</li> </ul>"},{"location":"explanation/file-reading/","title":"File Reading","text":"<p>Here's how the workflow unfolds:</p> In\u00a0[1]: Copied! <pre># Load the data into a DataFrame\ndf1 = read_df('\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx', header=[0, 1, 2])\ndf1.head(5)\n</pre> # Load the data into a DataFrame df1 = read_df('\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx', header=[0, 1, 2]) df1.head(5) Out[1]: \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868 \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf \u7d2f\u8ba1\u4ea7\u91cf \u8017\u8d39\u5de5\u65f6 Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 \u9884\u8ba1 \u5b9e\u9645 Unnamed: 6_level_2 \u672c\u65e5 \u7d2f\u8ba1 0 2007-08-10 00:00:00 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 10.0 20.0 1 2007-08-11 00:00:00 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000.0 40000 44000 82000 9.0 18.0 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 9.0 18.0 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 9.0 18.0 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000.0 40000 45000 83000 10.0 20.0 <p>The file is riddled with merged cells, empty rows, and redundant formatting that make it incompatible with pandas. If you try to load this file directly, pandas might misinterpret the structure or fail to parse it entirely.</p> <p>With our normalization feature, irregular datasets can be seamlessly transformed into clean, structured formats. When using the <code>create_tablegpt_agent</code> method, simply pass the <code>normalize_llm</code> parameter. The system will automatically analyze the irregular data and generate the appropriate transformation code, ensuring the dataset is prepared in the optimal format for further analysis.</p> <p>Below is an example of the code generated for the provided irregular dataset:</p> In\u00a0[2]: Copied! <pre># Normalize the data\ntry:\n    df = df1.copy()\n\n    import pandas as pd\n\n    # Assuming the original data is loaded into a DataFrame named df\n    # Here is the transformation process:\n\n    # Step 1: Isolate the Table Header\n    # Remove the unnecessary top rows and columns\n    final_df = df.iloc[2:, :9].copy()\n\n    # Step 2: Rename Columns of final_df\n    # Adjust the column names to match the desired format\n    final_df.columns = ['\u751f\u4ea7\u65e5\u671f', '\u5236\u9020\u7f16\u53f7', '\u4ea7\u54c1\u540d\u79f0', '\u9884\u5b9a\u4ea7\u91cf', '\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1', '\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645', '\u7d2f\u8ba1\u4ea7\u91cf', '\u672c\u65e5\u8017\u8d39\u5de5\u65f6', '\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6']\n\n    # Step 3: Data Processing\n    # Ensure there are no NaN values and drop any duplicate rows if necessary\n    final_df.dropna(inplace=True)\n    final_df.drop_duplicates(inplace=True)\n\n    # Convert the appropriate columns to numeric types\n    final_df['\u9884\u5b9a\u4ea7\u91cf'] = final_df['\u9884\u5b9a\u4ea7\u91cf'].astype(int)\n    final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'] = final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'].astype(int)\n    final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'] = final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'].astype(int)\n    final_df['\u7d2f\u8ba1\u4ea7\u91cf'] = final_df['\u7d2f\u8ba1\u4ea7\u91cf'].astype(int)\n    final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'] = final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'].astype(int)\n    final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'] = final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'].astype(int)\n\n    # Display the transformed DataFrame\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\n        final_df = final_df.iloc[1:]\n\n    # reassign df1 with the formatted DataFrame\n    df1 = final_df\nexcept Exception as e:\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\n</pre> # Normalize the data try:     df = df1.copy()      import pandas as pd      # Assuming the original data is loaded into a DataFrame named df     # Here is the transformation process:      # Step 1: Isolate the Table Header     # Remove the unnecessary top rows and columns     final_df = df.iloc[2:, :9].copy()      # Step 2: Rename Columns of final_df     # Adjust the column names to match the desired format     final_df.columns = ['\u751f\u4ea7\u65e5\u671f', '\u5236\u9020\u7f16\u53f7', '\u4ea7\u54c1\u540d\u79f0', '\u9884\u5b9a\u4ea7\u91cf', '\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1', '\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645', '\u7d2f\u8ba1\u4ea7\u91cf', '\u672c\u65e5\u8017\u8d39\u5de5\u65f6', '\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6']      # Step 3: Data Processing     # Ensure there are no NaN values and drop any duplicate rows if necessary     final_df.dropna(inplace=True)     final_df.drop_duplicates(inplace=True)      # Convert the appropriate columns to numeric types     final_df['\u9884\u5b9a\u4ea7\u91cf'] = final_df['\u9884\u5b9a\u4ea7\u91cf'].astype(int)     final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'] = final_df['\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1'].astype(int)     final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'] = final_df['\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645'].astype(int)     final_df['\u7d2f\u8ba1\u4ea7\u91cf'] = final_df['\u7d2f\u8ba1\u4ea7\u91cf'].astype(int)     final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'] = final_df['\u672c\u65e5\u8017\u8d39\u5de5\u65f6'].astype(int)     final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'] = final_df['\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6'].astype(int)      # Display the transformed DataFrame     if final_df.columns.tolist() == final_df.iloc[0].tolist():         final_df = final_df.iloc[1:]      # reassign df1 with the formatted DataFrame     df1 = final_df except Exception as e:     # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.     print(f\"Reformat failed with error {e}, use the original DataFrame.\") <p>Using the generated transformation code, the irregular dataset is converted into a clean, structured format, ready for analysis:</p> In\u00a0[3]: Copied! <pre>df1.head(5)\n</pre> df1.head(5) Out[3]: \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 5 2007-08-15 00:00:00 FK-006 \u8354\u679d\u679c\u8089\u996e\u6599 100000 40000 44000 82000 10 20 6 2007-08-16 00:00:00 FK-007 \u6a31\u6843\u679c\u8089\u996e\u6599 100000 40000 46000 84000 9 18 In\u00a0[4]: Copied! <pre># Remove leading and trailing whitespaces in column names\ndf1.columns = df1.columns.str.strip()\n\n# Remove rows and columns that contain only empty values\ndf1 = df1.dropna(how='all').dropna(axis=1, how='all')\n\n# Get the basic information of the dataset\ndf1.info(memory_usage=False)\n</pre> # Remove leading and trailing whitespaces in column names df1.columns = df1.columns.str.strip()  # Remove rows and columns that contain only empty values df1 = df1.dropna(how='all').dropna(axis=1, how='all')  # Get the basic information of the dataset df1.info(memory_usage=False) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 18 entries, 2 to 19\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   \u751f\u4ea7\u65e5\u671f    18 non-null     object\n 1   \u5236\u9020\u7f16\u53f7    18 non-null     object\n 2   \u4ea7\u54c1\u540d\u79f0    18 non-null     object\n 3   \u9884\u5b9a\u4ea7\u91cf    18 non-null     int64 \n 4   \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1  18 non-null     int64 \n 5   \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645  18 non-null     int64 \n 6   \u7d2f\u8ba1\u4ea7\u91cf    18 non-null     int64 \n 7   \u672c\u65e5\u8017\u8d39\u5de5\u65f6  18 non-null     int64 \n 8   \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6  18 non-null     int64 \ndtypes: int64(6), object(3)</pre> In\u00a0[5]: Copied! <pre># Show the first 5 rows to understand the structure\ndf1.head(5)\n</pre> # Show the first 5 rows to understand the structure df1.head(5) Out[5]: \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2 2007-08-12 00:00:00 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 3 2007-08-13 00:00:00 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 4 2007-08-14 00:00:00 FK-005 \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 5 2007-08-15 00:00:00 FK-006 \u8354\u679d\u679c\u8089\u996e\u6599 100000 40000 44000 82000 10 20 6 2007-08-16 00:00:00 FK-007 \u6a31\u6843\u679c\u8089\u996e\u6599 100000 40000 46000 84000 9 18"},{"location":"explanation/file-reading/#file-reading","title":"File Reading\u00b6","text":"<p>When working with dataset files, maintaining a clear separation between file reading and data analysis workflows can significantly improve control and clarity. At TableGPT Agent, we've designed a robust and structured approach to handling file reading that empowers the LLM (Large Language Model) to effectively analyze dataset files without being overwhelmed by unnecessary details. This method not only enhances the LLM's ability to inspect the data but also ensures a smoother and more reliable data analysis process.</p> <p>Traditionally, allowing an LLM to directly inspect a dataset might involve simply calling the <code>df.head()</code> function to preview its content. While this approach suffices for straightforward use cases, it often lacks depth when dealing with more complex or messy datasets. To address this, we've developed a multi-step file reading workflow designed to deliver richer insights into the dataset structure while preparing it for advanced analysis.</p>"},{"location":"explanation/file-reading/#normalization-optional","title":"Normalization (Optional)\u00b6","text":"<p>Not all files are immediately suitable for direct analysis. Excel files, in particular, can pose challenges\u2014irregular formatting, merged cells, and inconsistent headers are just a few examples. To tackle these issues, we introduce an optional normalization step that preprocesses the data, transforming it into a format that is \u201cpandas-friendly.\u201d</p> <p>This step addresses the most common quirks in Excel files, such as non-standard column headers, inconsistent row structures, or missing metadata. By resolving these typical issues upfront, the data is transformed into a format that is 'pandas-friendly' ensuring smooth integration with downstream processes.</p> <p>Example Scenario:</p> <p>Imagine you have an Excel file that looks like this:</p>"},{"location":"explanation/file-reading/#dataset-structure-overview","title":"Dataset Structure Overview\u00b6","text":"<p>After normalization, the next step dives into the structural aspects of the dataset using the <code>df.info()</code> function. Unlike <code>df.head()</code>, which only shows a snippet of the data, <code>df.info()</code> provides a holistic view of the dataset\u2019s structure. Key insights include:</p> <ul> <li>Column Data Types: Helps identify numerical, categorical, or textual data at a glance.</li> <li>Non-Null Counts: Reveals the completeness of each column, making it easy to spot potential gaps or inconsistencies.</li> </ul> <p>By focusing on the foundational structure of the dataset, this step enables the LLM to better understand the quality and layout of the data, paving the way for more informed analyses.</p>"},{"location":"explanation/file-reading/#dataset-content-preview","title":"Dataset Content Preview\u00b6","text":"<p>Finally, we utilize the <code>df.head()</code> function to provide a visual preview of the dataset\u2019s content. This step is crucial for understanding the actual values within the dataset\u2014patterns, anomalies, or trends often become apparent here.</p> <p>The number of rows displayed (<code>n</code>) is configurable to balance between granularity and simplicity. For smaller datasets or detailed exploration, a larger <code>n</code> might be beneficial. However, for larger datasets, displaying too many rows could overwhelm the LLM with excessive details, detracting from the primary analytical objectives.</p>"},{"location":"explanation/file-reading/#why-this-matters","title":"Why This Matters\u00b6","text":"<p>This structured, multi-step approach is not just about processing data; it's about making the LLM smarter in how it interacts with datasets. By systematically addressing issues like messy formatting, structural ambiguity, and information overload, we ensure the LLM operates with clarity and purpose.</p> <p>The separation of file reading from analysis offers several advantages:</p> <ul> <li>Enhanced Accuracy: Preprocessing and structure-checking reduce the risk of errors in downstream analyses.</li> <li>Scalability: Handles datasets of varying complexity and size with equal efficiency.</li> <li>Transparency: Provides clear visibility into the dataset\u2019s structure, enabling better decision-making.</li> </ul> <p>By adopting this method, TableGPT Agent transforms the way dataset files are read and analyzed, offering a smarter, more controlled, and ultimately more user-friendly experience.</p>"},{"location":"howto/incluster-code-execution/","title":"Incluster Code Execution","text":"<p>The <code>tablegpt-agent</code> directs <code>tablegpt</code> to generate Python code for data analysis. This code is then executed within a sandbox environment to ensure system security. The execution is managed by the pybox library, which provides a simple way to run Python code outside the main process.</p>"},{"location":"howto/incluster-code-execution/#usage","title":"Usage","text":"<p>If you're using the local executor (pybox.LocalPyBoxManager), follow these steps to configure the environment:</p> <ol> <li> <p>Install the dependencies required for the <code>IPython Kernel</code> using the following command:</p> <pre><code>pip install -r ipython/requirements.txt\n</code></pre> </li> <li> <p>Copy the code from the <code>ipython/ipython-startup-scripts</code> folder to the <code>$HOME/.ipython/profile_default/startup/</code> directory.</p> <p>This folder contains the functions and configurations needed to perform data analysis with <code>tablegpt-agent</code>.</p> <p>Note: The <code>~/.ipython</code> directory must be writable for the process launching the kernel, otherwise there will be a warning message: <code>UserWarning: IPython dir '/home/jovyan/.ipython' is not a writable location, using a temp directory.</code> and the startup scripts won't take effects.</p> </li> </ol>"},{"location":"howto/normalize-datasets/","title":"Normalize Datasets","text":"<p>!!! Note: When the <code>tablegpt-agent</code> enables the <code>Dataset Normalizer</code> to format the dataset, the dataset reading process will be noticeably slower. This is because the <code>Dataset Normalizer</code> needs to analyze the dataset and generate transformation code, a process that takes considerable time.</p> <p>It is worth noting that the data normalization process can effectively address most common data irregularities. However, for more complex datasets, further optimization may be needed, and the results depend on the specific normalization model used.</p> In\u00a0[7]: Copied! <pre>from pathlib import Path\nfrom langchain_openai import ChatOpenAI\nfrom pybox import LocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\nnormalize_llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_VLLM_MODEL_NAME\")\npybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    normalize_llm=normalize_llm,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n</pre> from pathlib import Path from langchain_openai import ChatOpenAI from pybox import LocalPyBoxManager from tablegpt.agent import create_tablegpt_graph from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") normalize_llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_VLLM_MODEL_NAME\") pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     normalize_llm=normalize_llm,     session_id=\"some-session-id\", # This is required when using file-reading ) <p>Given an Excel file \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx with merged cells and irregular headers:</p> \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868 \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf \u7d2f\u8ba1\u4ea7\u91cf \u8017\u8d39\u5de5\u65f6 \u9884\u8ba1 \u5b9e\u9645 \u672c\u65e5 \u7d2f\u8ba1 2007/8/10 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 2007/8/11 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000 40000 44000 82000 9 18 2007/8/12 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 2007/8/13 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 <p>Add the file for processing in the <code>additional_kwargs</code> of HumanMessage:</p> In\u00a0[8]: Copied! <pre>from typing import TypedDict\nfrom langchain_core.messages import HumanMessage\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\")]},\n)\n</pre> from typing import TypedDict from langchain_core.messages import HumanMessage  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\")]}, ) <p>Invoke the <code>tablegpt-agent</code> to normalize the datasets:</p> In\u00a0[9]: Copied! <pre>from datetime import date\nfrom tablegpt.agent.file_reading import Stage\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\n\nresponse[\"messages\"]\n</pre> from datetime import date from tablegpt.agent.file_reading import Stage  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, )  response[\"messages\"] Out[9]: <pre>[HumanMessage(content='', additional_kwargs={'attachments': [{'filename': '\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx'}]}, response_metadata={}, id='452ef08b-a45f-49ce-8489-4062cd35d57c'),\n AIMessage(content='\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002\\n```python\\n# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002', 'action': {'tool': 'python', 'tool_input': '# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)'}, 'model_type': None}, response_metadata={}, id='8e5d0026-215d-46e2-ab60-9174c5bf50bd', tool_calls=[{'name': 'python', 'args': {'query': '# Load the data into a DataFrame\\ndf = read_df(\\'\u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx\\')\\n# Normalize the data\\ntry:\\n    df = df.copy()\\n\\n    import pandas as pd\\n\\n    # Assuming the original data is loaded into a DataFrame named `df`\\n\\n    # Step 1: Isolate the Table Header\\n    # Remove the unnecessary top rows and columns\\n    df = df.drop([0, 1])  # Drop the first two rows which contain header information\\n    df = df.reset_index(drop=True)  # Reset the index to start from 0\\n\\n    # Step 2: Store the Result as `final_df`\\n    final_df = df\\n\\n    # Step 3: Rename Columns of final_df\\n    # Adjust the column names of final_df to match the desired format\\n    final_df.columns = [\\'\u751f\u4ea7\u65e5\u671f\\', \\'\u5236\u9020\u7f16\u53f7\\', \\'\u4ea7\u54c1\u540d\u79f0\\', \\'\u9884\u5b9a\u4ea7\u91cf\\', \\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\', \\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\', \\'\u7d2f\u8ba1\u4ea7\u91cf\\', \\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\', \\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\']\\n\\n    # Step 4: Data Processing\\n    # Ensure the data types are correct and remove any unnecessary rows or columns\\n    final_df = final_df.dropna(subset=[\\'\u751f\u4ea7\u65e5\u671f\\'])  # Remove rows where the production date is missing\\n    final_df = final_df.drop_duplicates()  # Remove duplicate rows if any\\n    final_df = final_df.reset_index(drop=True)  # Reset the index again to ensure it starts from 0\\n\\n    # Convert columns to appropriate data types\\n    final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'] = final_df[\\'\u9884\u5b9a\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u9884\u8ba1\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'] = final_df[\\'\u672c\u65e5\u4ea7\u91cf\u5b9e\u9645\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'] = final_df[\\'\u7d2f\u8ba1\u4ea7\u91cf\\'].astype(int)\\n    final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u672c\u65e5\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n    final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'] = final_df[\\'\u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\'].astype(int)\\n\\n    # Display the transformed DataFrame\\n    print(final_df)\\n    if final_df.columns.tolist() == final_df.iloc[0].tolist():\\n        final_df = final_df.iloc[1:]\\n\\n    # reassign df with the formatted DataFrame\\n    df = final_df\\nexcept Exception as e:\\n    # Unable to apply formatting to the original DataFrame. proceeding with the unformatted DataFrame.\\n    print(f\"Reformat failed with error {e}, use the original DataFrame.\")\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how=\\'all\\').dropna(axis=1, how=\\'all\\')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)'}, 'id': 'af99b549-09f1-4623-930a-3ffbfde40216', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': \"```pycon\\nReformat failed with error cannot convert float NaN to integer, use the original DataFrame.\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 26 entries, 0 to 25\\nData columns (total 9 columns):\\n #   Column  Non-Null Count  Dtype \\n---  ------  --------------  ----- \\n 0   \u751f\u4ea7\u65e5\u671f    22 non-null     object\\n 1   \u5236\u9020\u7f16\u53f7    25 non-null     object\\n 2   \u4ea7\u54c1\u540d\u79f0    20 non-null     object\\n 3   \u9884\u5b9a\u4ea7\u91cf    20 non-null     object\\n 4   \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1  21 non-null     object\\n 5   \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645  21 non-null     object\\n 6   \u7d2f\u8ba1\u4ea7\u91cf    25 non-null     object\\n 7   \u672c\u65e5\u8017\u8d39\u5de5\u65f6  20 non-null     object\\n 8   \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6  20 non-null     object\\ndtypes: object(9)\\n```\"}], name='python', id='58429576-461b-4a50-8c3d-5bbacca22cdb', tool_call_id='af99b549-09f1-4623-930a-3ffbfde40216', artifact=[]),\n AIMessage(content='\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002\\n```python\\n# Show the first 5 rows to understand the structure\\ndf.head(5)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002', 'action': {'tool': 'python', 'tool_input': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'model_type': None}, response_metadata={}, id='0c67c2b2-36d2-49cc-8fb8-7f6e5dfc8625', tool_calls=[{'name': 'python', 'args': {'query': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'id': 'a4ccd41a-a872-4ff0-aae5-678a96b9b54a', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n                  \u751f\u4ea7\u65e5\u671f    \u5236\u9020\u7f16\u53f7     \u4ea7\u54c1\u540d\u79f0    \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645   \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6\\n0  2007-08-10 00:00:00  FK-001  \u7315\u7334\u6843\u679c\u8089\u996e\u6599  100000  40000  45000  83000     10     20\\n1  2007-08-11 00:00:00  FK-002   \u897f\u74dc\u679c\u8089\u996e\u6599  100000  40000  44000  82000      9     18\\n2  2007-08-12 00:00:00  FK-003   \u8349\u8393\u679c\u8089\u996e\u6599  100000  40000  45000  83000      9     18\\n3  2007-08-13 00:00:00  FK-004   \u84dd\u8393\u679c\u8089\u996e\u6599  100000  40000  45000  83000      9     18\\n4  2007-08-14 00:00:00  FK-005  \u6c34\u5bc6\u6843\u679c\u8089\u996e\u6599  100000  40000  45000  83000     10     20\\n```'}], name='python', id='d828aa34-7c9e-4fee-8ae1-7b553530292b', tool_call_id='a4ccd41a-a872-4ff0-aae5-678a96b9b54a', artifact=[]),\n AIMessage(content='\u6211\u5df2\u7ecf\u4e86\u89e3\u4e86\u6570\u636e\u96c6 \u4ea7\u54c1\u751f\u4ea7\u7edf\u8ba1\u8868.xlsx \u7684\u57fa\u672c\u4fe1\u606f\u3002\u8bf7\u95ee\u6211\u53ef\u4ee5\u5e2e\u60a8\u505a\u4e9b\u4ec0\u4e48\uff1f', additional_kwargs={'parent_id': 'some-parent-id1'}, response_metadata={}, id='e836eba6-9597-4bf8-acfd-2a81871916a6')]</pre> <p>By formatting the content of the last <code>ToolMessage</code>, you can see the normalized data:</p> \u751f\u4ea7\u65e5\u671f \u5236\u9020\u7f16\u53f7 \u4ea7\u54c1\u540d\u79f0 \u9884\u5b9a\u4ea7\u91cf \u672c\u65e5\u4ea7\u91cf\u9884\u8ba1 \u672c\u65e5\u4ea7\u91cf\u5b9e\u9645 \u7d2f\u8ba1\u4ea7\u91cf \u672c\u65e5\u8017\u8d39\u5de5\u65f6 \u7d2f\u8ba1\u8017\u8d39\u5de5\u65f6 2007/8/10 FK-001 \u7315\u7334\u6843\u679c\u8089\u996e\u6599 100000 40000 45000 83000 10 20 2007/8/11 FK-002 \u897f\u74dc\u679c\u8089\u996e\u6599 100000 40000 44000 82000 9 18 2007/8/12 FK-003 \u8349\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18 2007/8/13 FK-004 \u84dd\u8393\u679c\u8089\u996e\u6599 100000 40000 45000 83000 9 18"},{"location":"howto/normalize-datasets/#normalize-datasets","title":"Normalize Datasets\u00b6","text":"<p>The Dataset Normalizer plugin is used to transform 'pandas-unfriendly' datasets (e.g., Excel files that do not follow a standard tabular structure) into a more suitable format for pandas. It is backed by an LLM that generates Python code to convert the original datasets into new ones.</p> <p>In <code>tablegpt-agent</code>, this plugin is used to better format 'pandas-unfriendly' datasets, making them more understandable for the subsequent steps. This plugin is optional; if used, it serves as the very first step in the File Reading Workflow, easing the difficulty of data analysis in the subsequent workflow.</p>"},{"location":"howto/normalize-datasets/#introduction","title":"Introduction\u00b6","text":"<p>The <code>Dataset Normalizer</code> is a specialized tool designed to tackle challenges that arise when working with irregular and poorly structured datasets. These challenges are especially prevalent in Excel files, which are often used as a flexible but inconsistent way of storing data.</p> <p>Analyzing Excel data files can pose significant challenges, such as:</p> <ul> <li>Irregular Formatting: Datasets may lack a consistent tabular structure, with varying cell sizes or non-standard layouts.</li> <li>Merged Cells: Cells spanning multiple rows or columns can disrupt parsing tools.</li> <li>Inconsistent Headers: Columns may have incomplete, redundant, or nested headers.</li> <li>Hidden Data: Data may be stored in additional sheets or rely on calculated fields that are not directly accessible.</li> <li>Mixed Data Types: Columns may contain inconsistent data types, such as numbers mixed with text.</li> <li>Empty or Placeholder Rows: Extra rows with missing or irrelevant data can complicate data loading and analysis.</li> </ul>"},{"location":"howto/normalize-datasets/#quick-start","title":"Quick Start\u00b6","text":"<p>To enable the <code>Dataset Normalizer</code>, ensure you pass it as a parameter when creating the <code>tablegpt-agent</code>. You can follow the example below:</p>"},{"location":"howto/persist-messages/","title":"Persist Messages","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import date\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom tablegpt.agent import create_tablegpt_graph\nfrom pybox import LocalPyBoxManager\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = LocalPyBoxManager()\ncheckpointer = MemorySaver()\n\ngraph = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n)\n</pre> from datetime import date  from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from tablegpt.agent import create_tablegpt_graph from pybox import LocalPyBoxManager  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = LocalPyBoxManager() checkpointer = MemorySaver()  graph = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer, ) <p>Conducting a Conversation with <code>TableGPT</code></p> In\u00a0[2]: Copied! <pre>resp = await graph.ainvoke(\n    input={\n        \"messages\": [(\"human\", \"Please introduce Jackie Chan\")],\n        \"parent_id\": \"1\",\n        \"date\": date.today(),\n    },\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\nresp[\"messages\"][-1]\n</pre> resp = await graph.ainvoke(     input={         \"messages\": [(\"human\", \"Please introduce Jackie Chan\")],         \"parent_id\": \"1\",         \"date\": date.today(),     },     config={\"configurable\": {\"thread_id\": \"1\"}}, ) resp[\"messages\"][-1] Out[2]: <pre>AIMessage(content=\"I understand that you're asking for an introduction to Jackie Chan. However, my primary role is to analyze datasets using Python. If you have a dataset related to Jackie Chan or any other topic, I'd be happy to help you analyze it. Could you please provide more details on what kind of data you have or what specific analysis you would like to perform?\", additional_kwargs={'parent_id': '1'}, response_metadata={}, id='cdf638ce-0e56-475b-a86b-0d8d7a0f6d05')</pre> <p>Continuing the Conversation</p> <p>To extend the conversation while maintaining context, you can provide new input along with the same <code>config</code> configuration:</p> <p>Note: <code>config</code> is the configuration associated with this <code>checkpointer</code>. Through this configuration, the <code>checkpointer</code> can retrieve previous status information, so that in subsequent conversations, the model can better understand the user's intention and reply.</p> In\u00a0[3]: Copied! <pre>resp = await graph.ainvoke(\n    input={\n        \"messages\": [(\"human\", \"Please name three movies he participated in.\")],\n        \"parent_id\": \"1\",\n        \"date\": date.today(),\n    },\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\nresp[\"messages\"][-1]\n</pre> resp = await graph.ainvoke(     input={         \"messages\": [(\"human\", \"Please name three movies he participated in.\")],         \"parent_id\": \"1\",         \"date\": date.today(),     },     config={\"configurable\": {\"thread_id\": \"1\"}}, ) resp[\"messages\"][-1] Out[3]: <pre>AIMessage(content=\"Certainly! Jackie Chan is a renowned actor, director, and martial artist, and he has starred in numerous films. Here are three popular movies in which he has participated:\\n\\n1. **Rush Hour (1998)** - In this action-comedy film, Jackie Chan plays the role of Inspector Lee, a Hong Kong detective who teams up with a Los Angeles detective, played by Chris Tucker, to solve a kidnapping case.\\n\\n2. **Drunken Master (1978)** - This is one of Jackie Chan's early films where he plays a young man who learns the art of drunken boxing to avenge his father's enemies.\\n\\n3. **The Karate Kid (2010)** - In this remake of the original 1984 film, Jackie Chan plays Mr. Han, a maintenance man who becomes the mentor to a young boy, Jaden Smith, teaching him martial arts and life lessons.\\n\\nIf you have any specific data or analysis related to these movies or Jackie Chan's filmography, feel free to provide more details, and I can help you with that!\", additional_kwargs={'parent_id': '1'}, response_metadata={}, id='4f62bec2-a4ec-43ad-97b2-cbade8c774b4')</pre> <p>Next, we demonstrates how to use <code>Postgres</code> as the backend for persisting checkpoint state using the langgraph-checkpoint-postgres library.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -U psycopg psycopg-pool psycopg_binary langgraph langgraph-checkpoint-postgres\n</pre> %pip install -U psycopg psycopg-pool psycopg_binary langgraph langgraph-checkpoint-postgres In\u00a0[4]: Copied! <pre>DB_URI = \"postgresql://postgres:postgres@127.0.0.1:5432/postgres?sslmode=disable\"\n</pre> DB_URI = \"postgresql://postgres:postgres@127.0.0.1:5432/postgres?sslmode=disable\" In\u00a0[5]: Copied! <pre>from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom tablegpt.agent import create_tablegpt_graph\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\n\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        checkpointer=checkpointer,\n    )\n    \n    res = await graph.ainvoke(\n        input={\n            \"messages\": [(\"human\", \"Who are you?\")],\n            \"parent_id\": \"2\",\n            \"date\": date.today()\n        },\n        config=config,\n    )\n    checkpoint_tuples = [c async for c in checkpointer.alist(config)]\n</pre> from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver from tablegpt.agent import create_tablegpt_graph  config = {\"configurable\": {\"thread_id\": \"2\"}}  async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:     graph = create_tablegpt_graph(         llm=llm,         pybox_manager=pybox_manager,         checkpointer=checkpointer,     )          res = await graph.ainvoke(         input={             \"messages\": [(\"human\", \"Who are you?\")],             \"parent_id\": \"2\",             \"date\": date.today()         },         config=config,     )     checkpoint_tuples = [c async for c in checkpointer.alist(config)] In\u00a0[6]: Copied! <pre>checkpoint_tuples\n</pre> checkpoint_tuples Out[6]: <pre>[CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6'}}, checkpoint={'v': 1, 'id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6', 'ts': '2024-11-19T08:56:48.239486+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}, 'data_analyze_graph': {'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000002.0.32407437506283565'}}, 'channel_versions': {'date': '00000000000000000000000000000003.0.1780977977687367', 'messages': '00000000000000000000000000000003.0.05509702188973753', '__start__': '00000000000000000000000000000002.3.0886787893869005e-05', 'parent_id': '00000000000000000000000000000003.0.43858547879187637', 'data_analyze_graph': '00000000000000000000000000000003.0.1082481333441786', 'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000003.0.593567034515958'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'data_analyze_graph': 'data_analyze_graph'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'data_analyze_graph': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, pending_writes=[]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-5d27-6b37-8002-4e274906f4a5'}}, checkpoint={'v': 1, 'id': '1efa6543-5d27-6b37-8002-4e274906f4a5', 'ts': '2024-11-19T08:56:48.222457+00:00', 'pending_sends': [], 'versions_seen': {'agent': {'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000003.0.8898909183470118'}, '__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'input_guard': {'start:input_guard': '00000000000000000000000000000002.0.6848611387807798'}, 'retrieve_columns': {'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'agent': '00000000000000000000000000000005.0.024232530645486583', 'messages': '00000000000000000000000000000005.0.14282746367420773', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'input_guard': '00000000000000000000000000000005.0.859473129275239', 'retrieve_columns': '00000000000000000000000000000005.0.7752176585300508', 'start:input_guard': '00000000000000000000000000000003.0.6183120254220215', 'start:retrieve_columns': '00000000000000000000000000000003.0.08187600687354024', 'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000004.0.5107824581933167'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'agent': 'agent', 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'join:input_guard+retrieve_columns:agent': set()}}, metadata={'step': 2, 'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')]}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a53-6aa4-8001-7474db32528e'}}, pending_writes=[]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a53-6aa4-8001-7474db32528e'}}, checkpoint={'v': 1, 'id': '1efa6543-4a53-6aa4-8001-7474db32528e', 'ts': '2024-11-19T08:56:46.248182+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'input_guard': {'start:input_guard': '00000000000000000000000000000002.0.6848611387807798'}, 'retrieve_columns': {'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'messages': '00000000000000000000000000000003.0.8478737633204881', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'input_guard': '00000000000000000000000000000003.0.06039244147872136', 'retrieve_columns': '00000000000000000000000000000003.0.8552403538042089', 'start:input_guard': '00000000000000000000000000000003.0.6183120254220215', 'start:retrieve_columns': '00000000000000000000000000000003.0.08187600687354024', 'join:input_guard+retrieve_columns:agent': '00000000000000000000000000000003.0.8898909183470118'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'input_guard': 'input_guard', 'retrieve_columns': 'retrieve_columns', 'join:input_guard+retrieve_columns:agent': {'input_guard', 'retrieve_columns'}}}, metadata={'step': 1, 'source': 'loop', 'writes': {'input_guard': {'messages': []}, 'retrieve_columns': {'messages': []}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2'}}, pending_writes=[('36d7d700-5338-feda-05f0-57d74fddbc0b', 'agent', 'agent'), ('36d7d700-5338-feda-05f0-57d74fddbc0b', 'messages', [AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')])]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2'}}, checkpoint={'v': 1, 'id': '1efa6543-4a4c-6639-8000-a5baf4d38bd2', 'ts': '2024-11-19T08:56:46.245208+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.2490273362085793', 'messages': '00000000000000000000000000000002.0.19507603965774079', '__start__': '00000000000000000000000000000002.0.18620036399153372', 'parent_id': '00000000000000000000000000000002.0.5201095646733788', 'start:input_guard': '00000000000000000000000000000002.0.6848611387807798', 'start:retrieve_columns': '00000000000000000000000000000002.0.030416452982199194'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'start:input_guard': '__start__', 'start:retrieve_columns': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b'}}, pending_writes=[('637b8d5c-1e9a-d15c-7560-eba440c88860', 'input_guard', 'input_guard'), ('637b8d5c-1e9a-d15c-7560-eba440c88860', 'messages', []), ('637b8d5c-1e9a-d15c-7560-eba440c88860', 'join:input_guard+retrieve_columns:agent', 'input_guard'), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'retrieve_columns', 'retrieve_columns'), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'messages', []), ('fcc182eb-567e-cef1-c5be-16b527e21434', 'join:input_guard+retrieve_columns:agent', 'retrieve_columns')]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'checkpoint_id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b'}}, checkpoint={'v': 1, 'id': '1efa6543-4a49-6ec0-bfff-7a6bdf830d6b', 'ts': '2024-11-19T08:56:46.244206+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.0.6163867462467301'}, 'channel_values': {'__start__': {'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}}}, metadata={'step': -1, 'source': 'input', 'writes': {'__start__': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2'}}, 'parents': {'': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}, 'thread_id': '2', 'checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'langgraph_node': 'data_analyze_graph', 'langgraph_path': ['__pregel_pull', 'data_analyze_graph'], 'langgraph_step': 1, 'langgraph_triggers': ['branch:__start__:router:data_analyze_graph'], 'langgraph_checkpoint_ns': 'data_analyze_graph:3d6fb60f-4da5-a1de-bcf2-fa5632547abd'}, parent_config=None, pending_writes=[('39da96de-984e-3d02-e2ef-9bd5146d7336', 'messages', [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')]), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'date', datetime.date(2024, 11, 19)), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'parent_id', '2'), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'start:input_guard', '__start__'), ('39da96de-984e-3d02-e2ef-9bd5146d7336', 'start:retrieve_columns', '__start__')]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, checkpoint={'v': 1, 'id': '1efa6543-4a1a-6852-8000-b975c65fb2ff', 'ts': '2024-11-19T08:56:46.224784+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}}, 'channel_versions': {'date': '00000000000000000000000000000002.0.602279509708772', 'messages': '00000000000000000000000000000002.0.47212683047327253', '__start__': '00000000000000000000000000000002.3.0886787893869005e-05', 'parent_id': '00000000000000000000000000000002.0.43326965052986344', 'branch:__start__:router:data_analyze_graph': '00000000000000000000000000000002.0.32407437506283565'}, 'channel_values': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930')], 'parent_id': '2', 'branch:__start__:router:data_analyze_graph': '__start__'}}, metadata={'step': 0, 'source': 'loop', 'writes': None, 'parents': {}, 'thread_id': '2'}, parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe'}}, pending_writes=[('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'data_analyze_graph', 'data_analyze_graph'), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'messages', [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')]), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'parent_id', '2'), ('3d6fb60f-4da5-a1de-bcf2-fa5632547abd', 'date', datetime.date(2024, 11, 19))]),\n CheckpointTuple(config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe'}}, checkpoint={'v': 1, 'id': '1efa6543-4a12-6bbb-bfff-ac4aee846bfe', 'ts': '2024-11-19T08:56:46.221598+00:00', 'pending_sends': [], 'versions_seen': {'__input__': {}}, 'channel_versions': {'__start__': '00000000000000000000000000000001.0.6926057190269731'}, 'channel_values': {'__start__': {'messages': [['human', 'Who are you?']], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}}}, metadata={'step': -1, 'source': 'input', 'writes': {'__start__': {'date': datetime.date(2024, 11, 19), 'messages': [['human', 'Who are you?']], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, parent_config=None, pending_writes=[('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'messages', [['human', 'Who are you?']]), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'parent_id', '2'), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'date', datetime.date(2024, 11, 19)), ('8e27b3ac-0a9d-697e-0667-6d3ccc170a50', 'branch:__start__:router:data_analyze_graph', '__start__')])]</pre> In\u00a0[7]: Copied! <pre>async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        checkpointer=checkpointer,\n    )\n    \n    graph_state = await graph.aget_state(config)\n</pre> async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:     graph = create_tablegpt_graph(         llm=llm,         pybox_manager=pybox_manager,         checkpointer=checkpointer,     )          graph_state = await graph.aget_state(config) In\u00a0[8]: Copied! <pre>graph_state\n</pre> graph_state Out[8]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2', 'date': datetime.date(2024, 11, 19)}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-5d51-63c3-8001-9bd42cf9d6e6'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'data_analyze_graph': {'date': datetime.date(2024, 11, 19), 'messages': [HumanMessage(content='Who are you?', additional_kwargs={}, response_metadata={}, id='32b67f59-d13e-43eb-9239-ec711811e930'), AIMessage(content=\"I am TableGPT2, an expert Python data analyst developed by Zhejiang University. My primary role is to assist you in analyzing datasets by writing Python code. I can help you with tasks such as data cleaning, transformation, visualization, and more. If you have a dataset or a specific analysis in mind, feel free to share it with me, and I'll do my best to help you!\", additional_kwargs={'parent_id': '2'}, response_metadata={}, id='560a88be-4fd0-4cc1-aa55-0747862fa222')], 'parent_id': '2'}}, 'parents': {}, 'thread_id': '2'}, created_at='2024-11-19T08:56:48.239486+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efa6543-4a1a-6852-8000-b975c65fb2ff'}}, tasks=())</pre>"},{"location":"howto/persist-messages/#persist-messages","title":"Persist Messages\u00b6","text":"<p>When creating TableGPT agents, you have the option to persist their state, enabling interactions with the agent across multiple sessions while retaining memory of previous interactions. For more information on persistence, you can refer to the Persistence documentation.</p> <p>The benefit of persistent messages is that you can interact with the TableGPT agent across multiple sessions, and the agent remembers previous interactions. This is useful for applications that require long-term tracking of context or complex conversations.</p> <p>TableGPT Agent leverages langgraph-checkpoint to implement persistent message storage. It supports using any type of <code>checkpointer</code> to store messages, such as: <code>Postgres</code>, <code>Redis</code>, <code>Memory</code>, etc. To integrate a checkpointer with a <code>TableGPT Agent</code>, you can follow the example below:</p>"},{"location":"howto/persist-messages/#installing-required-packages","title":"Installing Required Packages\u00b6","text":""},{"location":"howto/persist-messages/#use-async-connection","title":"Use Async Connection\u00b6","text":"<p>Note: <code>TableGPT Agent</code> is built based on LangGraph, and many of the <code>Node</code> components use <code>async/await</code> syntax, which does not yet support non-asynchronous operations.</p> <p>Setting up an asynchronous connection to the database allows for non-blocking database operations. This means other parts of your application can continue running while waiting for database operations to complete. This is particularly beneficial in high-concurrency scenarios or when dealing with I/O-bound operations.</p> <p>The <code>DB_URI</code> is the database connection URI, specifying the protocol for connecting to a PostgreSQL database, including authentication and the host where the database is running.</p>"},{"location":"howto/persist-messages/#creating-a-checkpointer-with-asyncpostgressaver","title":"Creating a Checkpointer with AsyncPostgresSaver\u00b6","text":"<p>This creates a connection based on a connection string:</p> <ul> <li>Advantages: Simplicity, encapsulates connection details</li> <li>Best for: Quick setup or when connection details are provided as a string</li> </ul>"},{"location":"howto/persist-messages/#get-persisted-messages-with-config","title":"Get Persisted Messages with Config\u00b6","text":"<p>We can use the same config parameters to retrieve persisted messages through the <code>checkpointer</code>. You can follow the example below:</p>"},{"location":"howto/retrieval/","title":"Enhance TableGPT Agent with RAG","text":"In\u00a0[3]: Copied! <pre>from langchain_core.vectorstores import InMemoryVectorStore\nfrom tablegpt.retriever import CSVLoader\n\nloader = CSVLoader(\"\u4ea7\u54c1\u9500\u91cf\u8868.csv\", autodetect_encoding=True)\n\ndocuments = []\nasync for item in loader.alazy_load():\n    documents.append(item)\n\n# Initialize with an embedding model\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n\nawait vector_store.aadd_documents(documents=documents)\ndataset_base_retriever = vector_store.as_retriever()\n</pre> from langchain_core.vectorstores import InMemoryVectorStore from tablegpt.retriever import CSVLoader  loader = CSVLoader(\"\u4ea7\u54c1\u9500\u91cf\u8868.csv\", autodetect_encoding=True)  documents = [] async for item in loader.alazy_load():     documents.append(item)  # Initialize with an embedding model vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())  await vector_store.aadd_documents(documents=documents) dataset_base_retriever = vector_store.as_retriever() In\u00a0[5]: Copied! <pre>from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\nfrom tablegpt.retriever import ColumnDocCompressor\n\ndataset_compressor = DocumentCompressorPipeline(\n    transformers=[ColumnDocCompressor()]\n)\n\ndataset_retriever = ContextualCompressionRetriever(\n    base_compressor=dataset_compressor,\n    base_retriever=dataset_base_retriever,\n)\n</pre> from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import DocumentCompressorPipeline from tablegpt.retriever import ColumnDocCompressor  dataset_compressor = DocumentCompressorPipeline(     transformers=[ColumnDocCompressor()] )  dataset_retriever = ContextualCompressionRetriever(     base_compressor=dataset_compressor,     base_retriever=dataset_base_retriever, ) In\u00a0[8]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom pybox import LocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    dataset_retriever=dataset_retriever,\n)\n</pre> from langchain_openai import ChatOpenAI from pybox import LocalPyBoxManager from tablegpt.agent import create_tablegpt_graph from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     dataset_retriever=dataset_retriever, ) <p>With this setup, your <code>TableGPT Agent</code> is ready to process user queries, retrieve relevant data, and generate contextually accurate responses. The integration of RAG techniques ensures that the agent leverages external data effectively, providing enhanced insights and performance.</p> In\u00a0[9]: Copied! <pre>from datetime import date\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(content=\"\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?\")\n\n_input = {\n    \"messages\": [message],\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),\n}\n\nresponse = await agent.ainvoke(_input)\n\nresponse[\"messages\"]\n</pre> from datetime import date from langchain_core.messages import HumanMessage  message = HumanMessage(content=\"\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?\")  _input = {     \"messages\": [message],     \"parent_id\": \"some-parent-id\",     \"date\": date.today(), }  response = await agent.ainvoke(_input)  response[\"messages\"] Out[9]: <pre>[HumanMessage(content='\u6843\u9165\u7684\u9500\u552e\u91cf\u662f\u591a\u5c11?', additional_kwargs={}, response_metadata={}, id='b567e1c3-8943-453c-9ebe-fa8d34cfc388'),\n SystemMessage(content='\\nHere are some extra column information that might help you understand the dataset:\\n- \u4ea7\u54c1\u9500\u91cf\u8868.csv:\\n  - {\"column\": \u540d\u79f0, \"dtype\": \"string\", \"values\": [\"\u82b1\u751f\u6843\u9165\", ...]}\\n  - {\"column\":  \u9500\u552e\u989d , \"dtype\": \"string\", \"values\": [\" \uffe5931,000.00 \", \" \uffe5225,060.00 \", \" \uffe558,500.00 \", ...]}\\n', additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='07fdddf4-05e8-4022-9a78-98ee3744aab2'),\n AIMessage(content=\"\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u8bfb\u53d6\u6587\u4ef6`\u4ea7\u54c1\u9500\u91cf\u8868.csv`\uff0c\u7136\u540e\u627e\u5230\u5217\u540d\u5305\u542b\u201c\u540d\u79f0\u201d\u548c\u201c\u9500\u552e\u989d\u201d\u7684\u5217\uff0c\u7279\u522b\u662f\u9700\u8981\u627e\u5230\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u3002\u8ba9\u6211\u4eec\u5148\u8bfb\u53d6\u6570\u636e\u5e76\u67e5\u770b\u524d\u51e0\u884c\u3002\\n```python\\nimport pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\\n```\", additional_kwargs={'thought': '\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u8bfb\u53d6\u6587\u4ef6`\u4ea7\u54c1\u9500\u91cf\u8868.csv`\uff0c\u7136\u540e\u627e\u5230\u5217\u540d\u5305\u542b\u201c\u540d\u79f0\u201d\u548c\u201c\u9500\u552e\u989d\u201d\u7684\u5217\uff0c\u7279\u522b\u662f\u9700\u8981\u627e\u5230\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u3002\u8ba9\u6211\u4eec\u5148\u8bfb\u53d6\u6570\u636e\u5e76\u67e5\u770b\u524d\u51e0\u884c\u3002', 'action': {'tool': 'python', 'tool_input': \"import pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\"}, 'parent_id': 'some-parent-id'}, response_metadata={}, id='27da6f10-2201-4349-bc23-9f7b42f34742', tool_calls=[{'name': 'python', 'args': {'query': \"import pandas as pd\\n\\n# \u8bfb\u53d6\u6570\u636e\\ndf = read_df(uri='\u4ea7\u54c1\u9500\u91cf\u8868.csv')\\n\\n# \u663e\u793a\u6570\u636e\u6846\u7684\u524d\u51e0\u884c\\ndf.head()\"}, 'id': 'be9a29de-7f5d-4010-a85b-37286ab99e86', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n       \u7f16\u53f7      \u540d\u79f0 \u5355\u4f4d   \u5355\u4ef7\uff08\u5143\uff09      \u9500\u552e\u91cf             \u9500\u552e\u989d \\n0  mb2033    \u6cd5\u5f0f\u9762\u5305  \u5305   \uffe57.40   305080   \uffe52,257,592.00 \\n1  mb2034    \u5976\u6614\u86cb\u7cd5  \u5305   \uffe55.80    93200     \uffe5540,560.00 \\n2  mb2035  \u5976\u6cb9\u5939\u5fc3\u997c\u5e72  \u5305   \uffe53.10   215300     \uffe5667,430.00 \\n3  mb2036     \u8471\u6cb9\u997c  \u5305   \uffe52.20   102300     \uffe5225,060.00 \\n4  mb2037    \u82b1\u751f\u6843\u9165  \u5305   \uffe53.80   130000     \uffe5494,000.00 \\n```'}], name='python', id='a48d70fd-2e01-48ee-a9a5-25dc0eec04d6', tool_call_id='be9a29de-7f5d-4010-a85b-37286ab99e86', artifact=[]),\n AIMessage(content='\u4ece\u6570\u636e\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u201c\u82b1\u751f\u6843\u9165\u201d\u7684\u9500\u552e\u91cf\u4e3a130,000\u5305\u3002', additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='5c5b703d-2eea-444b-a627-0828dca06df2')]</pre> <p>Output:</p> <p>Here are some extra column information that might help you understand the dataset:</p> <ul> <li>\u4ea7\u54c1\u9500\u91cf\u8868.csv:</li> <li>{\"column\": \u540d\u79f0, \"dtype\": \"string\", \"values\": [\"\u82b1\u751f\u6843\u9165\", ...]}</li> <li>{\"column\":  \u9500\u552e\u989d , \"dtype\": \"string\", \"values\": [\" \uffe5931,000.00 \", \" \uffe5225,060.00 \", \" \uffe558,500.00 \", ...]}</li> </ul> <p>The output confirms that the RAG approach effectively enriches the agent's responses by incorporating dataset context. This improvement allows the agent to provide detailed, actionable insights rather than generic answers, thereby enhancing its utility for complex queries.</p>"},{"location":"howto/retrieval/#enhance-tablegpt-agent-with-rag","title":"Enhance TableGPT Agent with RAG\u00b6","text":"<p>While the File Reading Workflow is adequate for most scenarios, it may not always provide the information necessary for the LLM to generate accurate code. Consider the following examples:</p> <ul> <li>A categorical column in the dataset contains 'foo', 'bar', and 'baz', but 'baz' only appears after approximately 100 rows. In this case, the LLM may not encounter the 'baz' value through <code>df.head()</code>.</li> <li>The user's query may not align with the dataset's content for several reasons:<ul> <li>The dataset lacks proper governance. For instance, a cell value might be misspelled from 'foo' to 'fou'.</li> <li>There could be a typo in the user's query. For example, if the user queries, \"Show me the data for 'fou',\" but the dataset contains 'foo' instead.</li> </ul> </li> </ul> <p>In such situations, the Dataset Retriever plugin can be utilized to fetch additional information about the dataset from external sources, thereby providing the LLM with more context and improving its ability to generate accurate responses.</p>"},{"location":"howto/retrieval/#quick-start","title":"Quick Start\u00b6","text":"<p>To help you quickly integrate and utilize <code>RAG</code> with the <code>TableGPT Agent</code>, follow the steps outlined in this section. These instructions will guide you through the process of loading datasets, enhancing retrieval with document compression, and integrating with a powerful LLM-based agent. By the end of this quick start, you'll be able to issue complex queries and receive enriched, context-aware responses.</p>"},{"location":"howto/retrieval/#step-1-install-required-dependencies","title":"Step 1: Install Required Dependencies\u00b6","text":"<p>To get started with using RAG in the TableGPT Agent, you need to install the necessary dependencies. The primary package required is langchain, which facilitates building retrieval-augmented workflows.</p> <p>Run the following command to install it:</p> <pre>pip install langchain\n</pre>"},{"location":"howto/retrieval/#step-2-load-and-prepare-data-with-csvloader","title":"Step 2: Load and Prepare Data with CSVLoader\u00b6","text":"<p>The <code>TableGPT Agent</code> provides a convenient <code>CSVLoader</code> for converting <code>CSV</code> or <code>Excel</code> files into a format that can be processed by the RAG pipeline. This method allows seamless integration of your data for further retrieval and embedding.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-3-build-a-context-aware-retriever-with-document-compression","title":"Step 3: Build a Context-Aware Retriever with Document Compression\u00b6","text":"<p>To enhance the retrieval process, <code>langchain</code> provides powerful retriever utilities that can be combined with custom compressors. In this step, we utilize the <code>ColumnDocCompressor</code> from tablegpt to focus on relevant columns and build an efficient <code>dataset_retriever</code>.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-4-integrate-with-tablegpt-agent","title":"Step 4: Integrate with TableGPT Agent\u00b6","text":"<p>In this step, we integrate the <code>dataset_retriever</code> with the <code>TableGPT Agent</code> using an <code>LLM</code> and a local execution environment. This setup ensures that the agent can handle user queries effectively by leveraging both the LLM and retrieved dataset context.</p> <p>Example Code:</p>"},{"location":"howto/retrieval/#step-5-analyze-data-with-the-tablegpt-agent","title":"Step 5: Analyze Data with the TableGPT Agent\u00b6","text":"<p>Finally, you can use the <code>TableGPT Agent</code> to perform analysis by sending a query. The response can help determine whether retrieval-augmented generation (RAG) has provided enhanced results. Observing the returned information allows you to assess the accuracy and completeness of the generated response.</p> <p>Example Code:</p>"},{"location":"tutorials/chat-on-tabular-data/","title":"Chat on Tabular Data","text":"In\u00a0[1]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pybox import LocalPyBoxManager\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\nfrom tablegpt.agent import create_tablegpt_graph\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\ncheckpointer = MemorySaver()\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n</pre> from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from pybox import LocalPyBoxManager from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR from tablegpt.agent import create_tablegpt_graph  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR) checkpointer = MemorySaver()  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer,     session_id=\"some-session-id\", # This is required when using file-reading ) <p>Add the file for processing in the additional_kwargs of HumanMessage. Here's an example using the Titanic dataset.</p> In\u00a0[2]: Copied! <pre>from typing import TypedDict\nfrom langchain_core.messages import HumanMessage\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n)\n</pre> from typing import TypedDict from langchain_core.messages import HumanMessage  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]}, ) <p>Invoke the agent as shown in the quick start:</p> In\u00a0[3]: Copied! <pre>from datetime import date\nfrom tablegpt.agent.file_reading import Stage\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\nresponse[\"messages\"]\n</pre> from datetime import date from tablegpt.agent.file_reading import Stage  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, ) response[\"messages\"] Out[3]: <pre>[HumanMessage(content='', additional_kwargs={'attachments': [{'filename': 'titanic.csv'}]}, response_metadata={}, id='ab0a7157-ad7d-4de8-9b24-1bee78ad7c55'),\n AIMessage(content=\"\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002\\n```python\\n# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\\n```\", additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002', 'action': {'tool': 'python', 'tool_input': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'model_type': None}, response_metadata={}, id='add6691d-d7ea-411d-9699-e99ae0b7de97', tool_calls=[{'name': 'python', 'args': {'query': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'id': 'b846aa01-04ef-4669-9a5c-53ddcb9a2dfb', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': \"```pycon\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 4 entries, 0 to 3\\nData columns (total 8 columns):\\n #   Column    Non-Null Count  Dtype  \\n---  ------    --------------  -----  \\n 0   Pclass    4 non-null      int64  \\n 1   Sex       4 non-null      object \\n 2   Age       4 non-null      float64\\n 3   SibSp     4 non-null      int64  \\n 4   Parch     4 non-null      int64  \\n 5   Fare      4 non-null      float64\\n 6   Embarked  4 non-null      object \\n 7   Survived  4 non-null      int64  \\ndtypes: float64(2), int64(4), object(2)\\n```\"}], name='python', id='0d441b21-bff3-463c-a07f-c0b12bd17bc5', tool_call_id='b846aa01-04ef-4669-9a5c-53ddcb9a2dfb', artifact=[]),\n AIMessage(content='\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002\\n```python\\n# Show the first 5 rows to understand the structure\\ndf.head(5)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002', 'action': {'tool': 'python', 'tool_input': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'model_type': None}, response_metadata={}, id='5e26ef1d-7042-471e-b39f-194a51a185c7', tool_calls=[{'name': 'python', 'args': {'query': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'id': 'f6be0d96-05b3-4b5b-8313-90197a8c3d87', 'type': 'tool_call'}]),\n ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n   Pclass     Sex   Age  SibSp  Parch    Fare Embarked  Survived\\n0       2  female  29.0      0      2  23.000        S         1\\n1       3  female  39.0      1      5  31.275        S         0\\n2       3    male  26.5      0      0   7.225        C         0\\n3       3    male  32.0      0      0  56.496        S         1\\n```'}], name='python', id='6fc6d8aa-546c-467e-91d3-d57b0b62dd68', tool_call_id='f6be0d96-05b3-4b5b-8313-90197a8c3d87', artifact=[]),\n AIMessage(content='\u6211\u5df2\u7ecf\u4e86\u89e3\u4e86\u6570\u636e\u96c6 titanic.csv \u7684\u57fa\u672c\u4fe1\u606f\u3002\u8bf7\u95ee\u6211\u53ef\u4ee5\u5e2e\u60a8\u505a\u4e9b\u4ec0\u4e48\uff1f', additional_kwargs={'parent_id': 'some-parent-id1'}, response_metadata={}, id='b6dc3885-94cb-4b0f-b691-f37c4c8c9ba3')]</pre> <p>Continue to ask questions for data analysis:</p> In\u00a0[4]: Copied! <pre>human_message = HumanMessage(content=\"How many men survived?\")\n\nasync for event in agent.astream_events(\n    input={\n        # After using checkpoint, you only need to add new messages here.\n        \"messages\": [human_message],\n        \"parent_id\": \"some-parent-id2\",\n        \"date\": date.today(),\n    },\n    version=\"v2\",\n    # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n    config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n):\n    event_name: str = event[\"name\"]\n    evt: str = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event_name == \"tools\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Other events can be handled here.\n        pass\n</pre> human_message = HumanMessage(content=\"How many men survived?\")  async for event in agent.astream_events(     input={         # After using checkpoint, you only need to add new messages here.         \"messages\": [human_message],         \"parent_id\": \"some-parent-id2\",         \"date\": date.today(),     },     version=\"v2\",     # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.     config={\"configurable\": {\"thread_id\": \"some-thread-id\"}}, ):     event_name: str = event[\"name\"]     evt: str = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event_name == \"tools\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Other events can be handled here.         pass  <pre>content=\"\u4e3a\u4e86\u56de\u7b54\u60a8\u7684\u95ee\u9898\uff0c\u6211\u5c06\u7b5b\u9009\u51fa\u6240\u6709\u7537\u6027\u4e58\u5ba2\u5e76\u8ba1\u7b97\u5176\u4e2d\u7684\u5e78\u5b58\u8005\u6570\u91cf\u3002\\n```python\\n# Filter male passengers who survived and count them\\nmale_survivors = df[(df['Sex'] == 'male') &amp; (df['Survived'] == 1)]\\nmale_survivors_count = male_survivors.shape[0]\\nmale_survivors_count\\n```\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-661d7496-341d-4a6b-84d8-b4094db66ef0'\ncontent=[{'type': 'text', 'text': '```pycon\\n1\\n```'}] name='python' id='1c7531db-9150-451d-a8dd-f07176454e6f' tool_call_id='2860e8bb-0fa7-421b-bb2d-bfeca873354b' artifact=[]\ncontent='\u6839\u636e\u6570\u636e\u96c6\uff0c\u6709 1 \u540d\u7537\u6027\u4e58\u5ba2\u5e78\u5b58\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-db640705-0085-4f47-adb4-3e0adce694cd'\n</pre>"},{"location":"tutorials/chat-on-tabular-data/#chat-on-tabular-data","title":"Chat on Tabular Data\u00b6","text":"<p>TableGPT Agent excels at analyzing and processing tabular data. To perform data analysis, you need to first let the agent \"see\" the dataset. This is done by a specific \"file-reading\" workflow. In short, you begin by \"uploading\" the dataset and let the agent read it. Once the data is read, you can ask the agent questions about it.</p> <p>To learn more about the file-reading workflow, see File Reading.</p> <p>For data analysis tasks, we introduce two important parameters when creating the agent: <code>checkpointer</code> and <code>session_id</code>.</p> <ul> <li>The <code>checkpointer</code> should be an instance of <code>langgraph.checkpoint.base.BaseCheckpointSaver</code>, which acts as a versioned \"memory\" for the agent. (See langgraph's persistence concept for more details.)</li> <li>The <code>session_id</code> is a unique identifier for the current session. It ties the agent's execution to a specific kernel, ensuring that the agent's results are retained across multiple invocations.</li> </ul>"},{"location":"tutorials/continue-analysis-on-generated-charts/","title":"Continue Analysis on Generated Charts","text":"In\u00a0[1]: Copied! <pre>from datetime import date\nfrom typing import TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pybox import LocalPyBoxManager\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt.agent.file_reading import Stage\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\ncheckpointer = MemorySaver()\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\", # This is required when using file-reading\n)\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n    filename: str\n\nattachment_msg = HumanMessage(\n    content=\"\",\n    # Please make sure your iPython kernel can access your filename.\n    additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n)\n\n# Reading and processing files.\nresponse = await agent.ainvoke(\n    input={\n        \"entry_message\": attachment_msg,\n        \"processing_stage\": Stage.UPLOADED,\n        \"messages\": [attachment_msg],\n        \"parent_id\": \"some-parent-id1\",\n        \"date\": date.today(),\n    },\n    config={\n        # Using checkpointer requires binding thread_id at runtime.\n        \"configurable\": {\"thread_id\": \"some-thread-id\"},\n    },\n)\n</pre> from datetime import date from typing import TypedDict  from langchain_core.messages import HumanMessage from langchain_openai import ChatOpenAI from langgraph.checkpoint.memory import MemorySaver from pybox import LocalPyBoxManager from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR from tablegpt.agent import create_tablegpt_graph from tablegpt.agent.file_reading import Stage  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR) checkpointer = MemorySaver()  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     checkpointer=checkpointer,     session_id=\"some-session-id\", # This is required when using file-reading )  class Attachment(TypedDict):     \"\"\"Contains at least one dictionary with the key filename.\"\"\"     filename: str  attachment_msg = HumanMessage(     content=\"\",     # Please make sure your iPython kernel can access your filename.     additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]}, )  # Reading and processing files. response = await agent.ainvoke(     input={         \"entry_message\": attachment_msg,         \"processing_stage\": Stage.UPLOADED,         \"messages\": [attachment_msg],         \"parent_id\": \"some-parent-id1\",         \"date\": date.today(),     },     config={         # Using checkpointer requires binding thread_id at runtime.         \"configurable\": {\"thread_id\": \"some-thread-id\"},     }, ) In\u00a0[2]: Copied! <pre># Define the human message that asks the model to draw a pie chart based on gender data\nhuman_message = HumanMessage(content=\"Draw a pie chart based on gender and the number of people of each gender.\")\n\nasync for event in agent.astream_events(\n    input={\n        \"messages\": [human_message],\n        \"parent_id\": \"some-parent-id2\",\n        \"date\": date.today(),\n    },\n    version=\"v2\",\n    # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n    config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n):\n    evt = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event[\"name\"] == \"tools\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Handle other events here\n        pass\n</pre> # Define the human message that asks the model to draw a pie chart based on gender data human_message = HumanMessage(content=\"Draw a pie chart based on gender and the number of people of each gender.\")  async for event in agent.astream_events(     input={         \"messages\": [human_message],         \"parent_id\": \"some-parent-id2\",         \"date\": date.today(),     },     version=\"v2\",     # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.     config={\"configurable\": {\"thread_id\": \"some-thread-id\"}}, ):     evt = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event[\"name\"] == \"tools\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Handle other events here         pass <pre>content=\"\u597d\u7684\uff0c\u6211\u5c06\u57fa\u4e8e\u6027\u522b\u7ed8\u5236\u4e00\u4e2a\u997c\u56fe\uff0c\u4ee5\u5c55\u793a\u6bcf\u4e2a\u6027\u522b\u7684\u4eba\u6570\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u7edf\u8ba1\u6bcf\u4e2a\u6027\u522b\u7684\u4eba\u6570\uff0c\u7136\u540e\u4f7f\u7528 `seaborn` \u548c `matplotlib` \u6765\u7ed8\u5236\u997c\u56fe\u3002\\n\\n```python\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Count the number of people for each gender\\ngender_counts = df['Sex'].value_counts()\\n\\n# Create a pie chart\\nplt.figure(figsize=(8, 6))\\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\\nplt.title('Gender Distribution')\\nplt.show()\\n```\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-6115fe22-3b55-4d85-be09-6c31a59736f6'\ncontent=[{'type': 'text', 'text': '```pycon\\n&lt;Figure size 800x600 with 1 Axes&gt;\\n```'}, {'type': 'image_url', 'image_url': {'url': 'data:image/png;base64,iVBORw0KG...'}}] name='python' id='226ba8f2-29a7-4706-9178-8cb5b4062488' tool_call_id='03eb1113-6aed-4e0a-a3c0-4cc0043a55ee' artifact=[]\ncontent='\u997c\u56fe\u5df2\u7ecf\u6210\u529f\u751f\u6210\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-83468bd1-9451-4c78-91a3-b0f96ffa169a'\n</pre> <p>Now let's set up the Visual Language Model (VLM) and create a new agent with VLM support:</p> In\u00a0[3]: Copied! <pre># Initialize the VLM instance\nvlm = ChatOpenAI(openai_api_base=\"YOUR_VLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_MODEL_NAME\")\n\n# Assume llm, pybox_manager, and memory_saver are defined elsewhere\nagent_with_vlm = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n    vlm=vlm,\n    checkpointer=checkpointer,\n    session_id=\"some-session-id\",\n)\n</pre> # Initialize the VLM instance vlm = ChatOpenAI(openai_api_base=\"YOUR_VLM_URL\", openai_api_key=\"whatever\", model_name=\"YOUR_MODEL_NAME\")  # Assume llm, pybox_manager, and memory_saver are defined elsewhere agent_with_vlm = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager,     vlm=vlm,     checkpointer=checkpointer,     session_id=\"some-session-id\", ) <p>We use a time travel feature to go back to before the last time the agent gave an answer, to avoid past memories hallucinating the model:</p> In\u00a0[4]: Copied! <pre>state_history = agent.get_state_history(config={\"configurable\": {\"thread_id\": \"some-thread-id\"}})\n\nto_replay = None\nfor state in list(state_history)[::-1]:\n    if state.next and state.next[0] == \"__start__\":\n        to_replay = state\n</pre> state_history = agent.get_state_history(config={\"configurable\": {\"thread_id\": \"some-thread-id\"}})  to_replay = None for state in list(state_history)[::-1]:     if state.next and state.next[0] == \"__start__\":         to_replay = state <p>Send the same question to the model via the new agent with VLM support</p> In\u00a0[5]: Copied! <pre>async for event in agent_with_vlm.astream_events(\n    None,\n    to_replay.config,\n    version=\"v2\",\n):\n    evt = event[\"event\"]\n    if evt == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n    elif event[\"name\"] == \"tools\" and evt == \"on_chain_stream\":\n        for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n            print(lc_msg)\n    else:\n        # Handle other events here\n        pass\n</pre> async for event in agent_with_vlm.astream_events(     None,     to_replay.config,     version=\"v2\", ):     evt = event[\"event\"]     if evt == \"on_chat_model_end\":         print(event[\"data\"][\"output\"])     elif event[\"name\"] == \"tools\" and evt == \"on_chain_stream\":         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:             print(lc_msg)     else:         # Handle other events here         pass <pre>content=\"\u597d\u7684\uff0c\u6211\u5c06\u7ed8\u5236\u4e00\u4e2a\u997c\u56fe\u6765\u5c55\u793a\u6570\u636e\u96c6\u4e2d\u7537\u6027\u548c\u5973\u6027\u4e58\u5ba2\u7684\u6570\u91cf\u3002\\n```python\\n# Count the number of passengers by gender\\ngender_counts = df['Sex'].value_counts()\\n\\n# Plot a pie chart\\nplt.figure(figsize=(8, 6))\\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140)\\nplt.title('Gender Distribution')\\nplt.show()\\n```\\n\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-2d05b2ab-32f4-481f-8fa5-43c78515d9c3'\ncontent=[{'type': 'text', 'text': '```pycon\\n&lt;Figure size 800x600 with 1 Axes&gt;\\n```'}, {'type': 'image_url', 'image_url': {'url': 'data:image/png;base64,iVBORw0K...'}}] name='python' id='51a99935-b0b1-496d-9a45-c1f318104773' tool_call_id='918d57ee-7362-4e0d-8d66-64b7e57ecaf6' artifact=[]\ncontent='\u997c\u56fe\u663e\u793a\u6570\u636e\u96c6\u4e2d\u6027\u522b\u5206\u5e03\u4e3a 50% \u5973\u6027\u548c 50% \u7537\u6027\uff0c\u8fd9\u8868\u660e\u7537\u6027\u548c\u5973\u6027\u4e58\u5ba2\u6570\u91cf\u76f8\u7b49\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'qwen2-vl-7b-instruct'} id='run-d9b0e891-f03c-40c8-8474-9fef7511c40b'\n</pre> <p>We observe that the answer provided by the agent with VLM support is significantly more detailed, including a comprehensive description of the generated images.</p>"},{"location":"tutorials/continue-analysis-on-generated-charts/#continue-analysis-on-generated-charts","title":"Continue Analysis on Generated Charts\u00b6","text":"<p>While TableGPT2 excels in data analysis tasks, it currently lacks built-in support for visual modalities. Many data analysis tasks involve visualization, so to address this limitation, we provide an interface for integrating your own Visual Language Model (VLM) plugin.</p> <p>When the agent performs a visualization task\u2014typically using <code>matplotlib.pyplot.show</code>\u2014the VLM will take over from the LLM, offering a more nuanced summarization of the visualization. This approach avoids the common pitfalls of LLMs in visualization tasks, which often either state, \"I have plotted the data,\" or hallucinating the content of the plot.</p> <p>We continue using the agent from the previous section to perform a data visualization task and observe its final output.</p> <p>NOTE Before you start, you can install Chinese fonts using the following command:</p> <pre>apt-get update &amp;&amp; apt-get install -y --no-install-recommends fonts-noto-cjk\nmplfonts init\n</pre>"},{"location":"tutorials/quick-start/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install tablegpt-agent\n</pre> %pip install tablegpt-agent <p>This package depends on pybox to manage code execution environment. By default, pybox operates in an in-cluster mode. If you intend to run tablegpt-agent in a local environment, install the optional dependency as follows:</p> In\u00a0[1]: Copied! <pre>%pip install tablegpt-agent[local]\n</pre> %pip install tablegpt-agent[local] <p>In the console or notebook, set the proxy as follows:</p> In\u00a0[2]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom pybox import LocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\nllm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\npybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\nagent = create_tablegpt_graph(\n    llm=llm,\n    pybox_manager=pybox_manager,\n)\n</pre> from langchain_openai import ChatOpenAI from pybox import LocalPyBoxManager from tablegpt.agent import create_tablegpt_graph from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR  llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\") pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)  agent = create_tablegpt_graph(     llm=llm,     pybox_manager=pybox_manager, ) <p>To interact with the agent:</p> In\u00a0[3]: Copied! <pre>from datetime import date\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(content=\"Hi\")\n\n_input = {\n    \"messages\": [message],\n    \"parent_id\": \"some-parent-id\",\n    \"date\": date.today(),\n}\n\nresponse = await agent.ainvoke(_input)\nresponse[\"messages\"]\n</pre> from datetime import date from langchain_core.messages import HumanMessage  message = HumanMessage(content=\"Hi\")  _input = {     \"messages\": [message],     \"parent_id\": \"some-parent-id\",     \"date\": date.today(), }  response = await agent.ainvoke(_input) response[\"messages\"] <pre>[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='34fe748c-81ab-49ea-bec6-9c621598a61a'), AIMessage(content=\"Hello! How can I assist you with data analysis today? Please let me know the details of the dataset you're working with and what specific analysis you'd like to perform.\", additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='a1ee29d2-723e-41c7-b420-27d0cfaed5dc')]\n</pre> <p>You can get more detailed outputs with the <code>astream_events</code> method:</p> In\u00a0[4]: Copied! <pre>async for event in agent.astream_events(\n    input=_input,\n    version=\"v2\",\n):\n    # We ignore irrelevant events here.\n    if event[\"event\"] == \"on_chat_model_end\":\n        print(event[\"data\"][\"output\"])\n</pre> async for event in agent.astream_events(     input=_input,     version=\"v2\", ):     # We ignore irrelevant events here.     if event[\"event\"] == \"on_chat_model_end\":         print(event[\"data\"][\"output\"]) <pre>content='Hello! How can I assist you with your data analysis today? Please let me know what dataset you are working with and what specific analyses or visualizations you would like to perform.' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-525eb149-0e3f-4b04-868b-708295f789ac'\n</pre>"},{"location":"tutorials/quick-start/#quickstart","title":"Quickstart\u00b6","text":""},{"location":"tutorials/quick-start/#installation","title":"Installation\u00b6","text":"<p>To install TableGPT Agent, use the following command:</p>"},{"location":"tutorials/quick-start/#setup-the-llm-service","title":"Setup the LLM Service\u00b6","text":"<p>Before using TableGPT Agent, ensure you have an OpenAI-compatible server configured to host TableGPT2. We recommend using vllm for this:</p> <pre>python -m vllm.entrypoints.openai.api_server --served-model-name TableGPT2-7B --model path/to/weights\n</pre> <p>Notes:</p> <ul> <li>To analyze tabular data with <code>tablegpt-agent</code>, make sure <code>TableGPT2</code> is served with <code>vllm</code> version 0.5.5 or higher.</li> <li>For production environments, it's important to optimize the vllm server configuration. For details, refer to the vllm documentation on server configuration.</li> </ul>"},{"location":"tutorials/quick-start/#chat-with-tablegpt-agent","title":"Chat with TableGPT Agent\u00b6","text":"<p>To create an agent, you'll need at least an <code>LLM</code> instance and a <code>PyBoxManager</code>:</p> <p>NOTE 1: This tutorial uses <code>langchain-openai</code> for the llm instance. Please install it first.</p> <pre>pip install langchain-openai\n</pre> <p>NOTE 2: TableGPT Agent fully supports aync invocation. To start a Python console that supports asynchronous operations, run the following command:</p> <pre>python -m asyncio\n</pre>"}]}
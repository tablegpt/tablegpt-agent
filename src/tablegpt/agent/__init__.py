from __future__ import annotations

from datetime import date  # noqa: TCH003
from typing import TYPE_CHECKING

from langchain_core.messages import BaseMessage  # noqa: TCH002
from langgraph.graph import END, START, MessagesState, StateGraph

from tablegpt.agent.data_analyzer import create_data_analyze_workflow
from tablegpt.agent.file_reading import Stage, create_file_reading_workflow

if TYPE_CHECKING:
    from pathlib import Path

    from langchain_core.language_models import BaseLanguageModel
    from langchain_core.retrievers import BaseRetriever
    from langchain_core.runnables import Runnable
    from langgraph.checkpoint.base import BaseCheckpointSaver
    from langgraph.graph.state import CompiledStateGraph
    from pybox.base import BasePyBoxManager


class AgentState(MessagesState):
    # This is a bit of a hack to pass parent id to the agent state
    # But it act as the group id of all messages generated by the agent
    # This will be used in subgraphs
    parent_id: str | None
    # Current Date
    date: date
    # The message that we received from the user, act as an entry point
    entry_message: BaseMessage
    processing_stage: Stage


def create_tablegpt_graph(
    llm: BaseLanguageModel,
    pybox_manager: BasePyBoxManager,
    *,
    workdir: Path | None = None,
    model_type: str | None = None,
    vlm: BaseLanguageModel | None = None,
    session_id: str | None = None,
    error_trace_cleanup: bool = False,
    guard_chain: Runnable | None = None,
    dataset_retriever: BaseRetriever | None = None,
    nlines: int | None = None,
    enable_normalization: bool = False,
    normalize_llm: BaseLanguageModel | None = None,
    checkpointer: BaseCheckpointSaver | None = None,
    verbose: bool = False,
) -> CompiledStateGraph:
    """_summary_

    Args:
        llm (BaseLanguageModel): _description_
        pybox_manager (BasePyBoxManager): _description_
        workdir (Path | None, optional): _description_. Defaults to None.
        model_type (str | None, optional): _description_. Defaults to None.
        vlm (BaseLanguageModel | None, optional): _description_. Defaults to None.
        session_id (str | None, optional): _description_. Defaults to None.
        error_trace_cleanup (bool, optional): _description_. Defaults to False.
        guard_chain (Runnable | None, optional): _description_. Defaults to None.
            Your guard chain should return a tuple of (safety_flag, risk_category).
            The safety flag should be one of "safe", "unsafe", or "unknown".
            The risk category should be one of the hazard categories defined in the guard.py file.
        dataset_retriever (BaseRetriever | None, optional): _description_. Defaults to None.
        nlines (int | None, optional): _description_. Defaults to None.
        enable_normalization (bool, optional): _description_. Defaults to False.
        normalize_llm (BaseLanguageModel | None, optional): _description_. Defaults to None.
        checkpointer (BaseCheckpointSaver | None, optional): _description_. Defaults to None.
        verbose (bool, optional): _description_. Defaults to False.

    Returns:
        CompiledStateGraph: _description_
    """
    workflow = StateGraph(AgentState)
    file_reading_graph = create_file_reading_workflow(
        nlines=nlines,
        pybox_manager=pybox_manager,
        workdir=workdir,
        session_id=session_id,
        model_type=model_type,
        enable_normalization=enable_normalization,
        normalize_llm=normalize_llm,
        verbose=verbose,
    )
    data_analyze_graph = create_data_analyze_workflow(
        llm=llm,
        pybox_manager=pybox_manager,
        workdir=workdir,
        vlm=vlm,
        session_id=session_id,
        error_trace_cleanup=error_trace_cleanup,
        guard_chain=guard_chain,
        dataset_retriever=dataset_retriever,
        verbose=verbose,
    )

    def router(state: AgentState) -> str:
        # Must have at least one message when entering this router
        last_message = state["messages"][-1]
        if last_message.additional_kwargs.get("attachments"):
            return "file_reading_graph"
        return "data_analyze_graph"

    workflow.add_node("file_reading_graph", file_reading_graph)
    workflow.add_node("data_analyze_graph", data_analyze_graph)

    workflow.add_conditional_edges(START, router)
    workflow.add_edge("file_reading_graph", END)
    workflow.add_edge("data_analyze_graph", END)

    return workflow.compile(checkpointer=checkpointer, debug=verbose)
